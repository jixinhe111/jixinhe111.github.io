<!DOCTYPE html>
<html lang="zh-cn" >
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content="jxh"/>

  
  <meta name="description" content="RockerMQ 对于上图中几个角色的说明： （1）NameServer：RocketMQ集群的命名服务器（也可以说是注册中心），它本身是无状态的（实际情况下可能存在每个NameServer实例上的数据有短暂的不一致现"/>
  

  
  
  <meta name="keywords" content="Hugo, theme, zozo"/>
  

  
  <link rel="canonical" href="https://jixinhe111.github.io/posts/rockermq/"/>

  

  <title>RockerMQ &middot; 两只老虎 --from jxh</title>

  <link rel="shortcut icon" href="https://jixinhe111.github.io/images/favicon.ico"/>
  <link rel="stylesheet" href="https://jixinhe111.github.io/css/animate.min.css"/>
  <link rel="stylesheet" href="https://jixinhe111.github.io/css/remixicon.css"/>
  <link rel="stylesheet" href="https://jixinhe111.github.io/css/zozo.css"/>
  <link rel="stylesheet" href="https://jixinhe111.github.io/css/highlight.css"/>

  
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="/">首页</a>
      </li>
      
      <li>
        <a href="/posts/">归档</a>
      </li>
      
      <li>
        <a href="/tags/">标签</a>
      </li>
      
      <li>
        <a href="/about/">关于</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="https://jixinhe111.github.io/">
          <span>两只老虎 --from jxh</span>
          <img src="https://jixinhe111.github.io/images/logo.svg"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title">一只没有眼睛，一只没有耳朵</p>
      <div class="my_socials">
        
        
        <a href="%20" title="facebook" target="_blank"><i class="remixicon-facebook-fill"></i></a>
        
        
        
        <a href="%20" title="github" target="_blank"><i class="remixicon-github-fill"></i></a>
        
        
        
        <a href="%20" title="instagram" target="_blank"><i class="remixicon-instagram-fill"></i></a>
        
        
        
        <a href="%20" title="twitter" target="_blank"><i class="remixicon-twitter-fill"></i></a>
        
        
        
        <a href="%20" title="weibo" target="_blank"><i class="remixicon-weibo-fill"></i></a>
        
        
      </div>
    </div>
  </div>
</div>

  <div class="content">
    <div class="post_page">
      <div class="post animated fadeInDown">
        <div class="post_title post_detail_title">
          <h2><a href='/posts/rockermq/'>RockerMQ</a></h2>
          <span class="date">2020.06.30</span>
        </div>
        <div class="post_content markdown">

<h3 id="rockermq">RockerMQ</h3>

<p><img src="https://upload-images.jianshu.io/upload_images/12619159-a858d38e0b38c406.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" /></p>

<p><strong>对于上图中几个角色的说明：</strong>
 （1）<strong>NameServer</strong>：RocketMQ集群的命名服务器（也可以说是注册中心），它本身是无状态的（实际情况下可能存在每个NameServer实例上的数据有短暂的不一致现象，但是通过定时更新，在大部分情况下都是一致的），用于管理集群的元数据（ 例如，KV配置、Topic、Broker的注册信息）。
 （2）<strong>Broker（Master）</strong>：RocketMQ消息代理服务器主节点，起到串联Producer的消息发送和Consumer的消息消费，和将消息的落盘存储的作用；
 （3）<strong>Broker（Slave）</strong>：RocketMQ消息代理服务器备份节点，主要是通过<strong>同步/异步</strong>的方式将主节点的消息同步过来进行备份，为RocketMQ集群的高可用性提供保障；
 （4）<strong>Producer（消息生产者）</strong>：在这里为普通消息的生产者，主要基于RocketMQ-Client模块将消息发送至RocketMQ的主节点。
 <strong>对于上面图中几条通信链路的关系：</strong>
 （1）<strong>Producer与NamerServer</strong>：每一个Producer会与NameServer集群中的<strong>一个实例</strong>建立TCP连接，从这个NameServer实例上拉取Topic路由信息；
 （2）<strong>Producer和Broker</strong>:Producer会和它要发送的topic相关联的<strong>Master</strong>的Broker代理服务器建立TCP连接，用于<strong>发送消息</strong>以及定时的<strong>心跳信息</strong>；
 （3）<strong>Broker和NamerServer</strong>：Broker（Master or Slave）均会和<strong>每一个</strong>NameServer实例来建立TCP连接。Broker在启动的时候会注册自己配置的Topic信息到NameServer集群的<strong>每一台</strong>机器中。即每一个NameServer均有该broker的Topic路由配置信息。其中，Master与Master之间无连接，Master与Slave之间有连接；</p>

<h4 id="1-存储机制">1、存储机制</h4>

<p>RocketMQ 消息存储架构中的三大角色——CommitLog、ConsumeQueue 和 IndexFile。</p>

<ul>
<li>CommitLog：消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容，消息内容不是定长的。单个文件大小默认 <strong>1G</strong> ，文件名长度为 20 位，左边补零，剩余为起始偏移量，比如 00000000000000000000 代表了第一个文件，起始偏移量为 0，文件大小为 1G = 1073741824；当第一个文件写满了，第二个文件为 00000000001073741824，起始偏移量为 1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。</li>
<li>ConsumeQueue：消息消费队列，引入的目的主要是提高消息消费的性能，由于 RocketMQ 是基于主题 Topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 Topic 检索消息是非常低效的。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的<strong>起始物理偏移量 offset，消息大小 size 和消息 Tag 的 HashCode 值</strong>（用于过滤消息）。consumequeue 文件可以看成是基于 topic 的 commitlog <strong>索引文件</strong>，故 consumequeue 文件夹的组织方式如下：topic/queue/file 三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 consumequeue 文件采取定长设计，每一个条目共 20 个字节，分别为 8 字节的 commitlog 物理偏移量、4 字节的消息长度、8 字节 tag hashcode，单个文件由 30W 个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue 文件大小约 5.72M；</li>
<li>IndexFile：IndexFile（索引文件）提供了一种可以<strong>通过 key 或时间区间来查询</strong>消息的方法。这里只做科普不做详细介绍。</li>
</ul>

<p>总结来说，整个消息存储的结构，最主要的就是 CommitLog 和 ConsumeQueue。而 <strong>ConsumeQueue 你可以大概理解为 Topic 中的队列</strong>。</p>

<p><a href="http://dockone.io/uploads/article/20200211/25a00eaed9cc2c56a01ad8008b073712.jpg"><img src="http://dockone.io/uploads/article/20200211/25a00eaed9cc2c56a01ad8008b073712.jpg" alt="22.jpg" /></a></p>

<p>RocketMQ 采用的是混合型的存储结构，即为 Broker 单个实例下所有的队列<strong>共用一个日志数据</strong>文件来存储消息。有意思的是在同样高并发的 Kafka 中会为每个 partition 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，RockeMQ 是不分书的种类直接成批的塞上去的，而 Kafka 是将书本放入指定的分类区域的。</p>

<p>而 RocketMQ 为什么要这么做呢？原因是<strong>提高数据的写入效率</strong>，不分 Topic 意味着我们有更大的几率获取成批的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。</p>

<p>讲到这里，你可能对 RocketMQ 的存储架构还有些模糊，没事，我们结合着图来理解一下。</p>

<p><a href="http://dockone.io/uploads/article/20200211/2a8332a3e1a7235069fb170444e6701c.png"><img src="http://dockone.io/uploads/article/20200211/2a8332a3e1a7235069fb170444e6701c.png" alt="23.png" /></a></p>

<p>首先，在最上面的那一块就是我刚刚讲的你现在可以直接把 ConsumerQueue 理解为 Queue。</p>

<p>在图中最左边说明了红色方块代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 Topic、QueueId 和具体消息内容，而在 Broker 中管你是哪门子消息，他直接全部顺序存储到了 CommitLog。而根据生产者指定的 Topic 和 QueueId 将这条消息本身在 CommitLog 的偏移（offset），消息本身大小，和 tag 的 hash 值存入对应的 ConsumeQueue 索引文件中。而在每个队列中都保存了 ConsumeOffset 即每个消费者组的消费位置，而消费者拉取消息进行消费的时候只需要根据 ConsumeOffset 获取下一个未被消费的消息就行了。</p>

<p>这里不涉及到一些细节讨论，比如稀疏索引等等问题。</p>

<p>为什么 CommitLog 文件要设计成固定大小的长度呢？提醒：内存映射机制。</p>

<p><strong>Page Cache</strong></p>

<p>通常文件读写比较慢，如果对文件进行顺序读写，速度几乎是接近于内存的随机读写，为什么会这么快，原因就是Page Cache。</p>

<p>OS发现系统的物理内存有大量剩余时，为了提高IO的性能，就会使用多余的内存当做文件缓存，广义我们说的Page Cache就是这些内存的子集。</p>

<p>OS在读磁盘时会将当前区域的内容全部读到Cache中，以便下次读时能命中Cache，写磁盘时直接写到Cache中就写返回，由OS的pdflush以某些策略将Cache的数据Flush回磁盘。</p>

<p>但是系统上文件非常多，即使是多余的Page Cache也是非常宝贵的资源，OS不可能将Page Cache随机分配给任何文件，Linux底层就提供了mmap将一个程序指定的文件映射进虚拟内存（Virtual Memory），对文件的读写就变成了对内存的读写，能充分利用Page Cache。不过，文件IO仅仅用到了Page Cache还是不够的，如果对文件进行随机读写，会使虚拟内存产生很多缺页（Page Fault）中断。</p>

<p>Mmap</p>

<h4 id="2-producer">2、Producer</h4>

<p>默认生成消息的实现类—DefaultMQProducerImpl的启动主要流程如下：
 （1）初始化得到MQClientInstance实例对象，并注册至本地缓存变量—producerTable中；
 （2）将默认Topic（<strong>“TBW102”</strong>）保存至本地缓存变量—topicPublishInfoTable中；（ 生产者利用这个<strong>“TBW102”</strong>Topic来进行自动创建其他topic ）
 （3）MQClientInstance实例对象调用自己的start()方法，启动一些客户端本地的服务线程，如拉取消息服务、客户端网络通信服务、重新负载均衡服务以及其他若干个定时任务（包括，更新路由/清理下线Broker/发送心跳/持久化consumerOffset/调整线程池），并重新做一次启动（这次参数为false）；
 （4）最后向所有的Broker代理服务器节点发送心跳包；</p>

<p><strong>send发送方法的核心流程</strong>（SendResult sendResult = producer.send(msg);）</p>

<p>通过Rocketmq的客户端模块发送消息主要有以下三种方法：
（1）<strong>同步方式</strong>
（2）<strong>异步方式</strong>
（3）<strong>Oneway方式 : 只负责发送消息,不等待服务器回应且没有回调函数触发,即只发送请求</strong></p>

<p>1、尝试获取TopicPublishInfo的路由信息</p>

<p>我们一步步debug进去后会发现在sendDefaultImpl()方法中先对待发送的消息进行前置的验证。如果消息的Topic和Body均没有问题的话，那么会调用—tryToFindTopicPublishInfo()方法，根据待发送消息中包含的Topic尝试从Client端的<strong>本地缓存变量</strong>—<strong>topicPublishInfoTable</strong>中查找，如果没有则会从NameServer上更新Topic的路由信息（其中，调用了MQClientInstance的updateTopicRouteInfoFromNameServer方法，最终执行的是MQClientAPIImpl实例的getTopicRouteInfoFromNameServer方法），这里分别会存在以下两种场景：
 （1）<strong>生产者第一次向该topic发送消息（此时，Topic在NameServer中并不存在）</strong>：因为第一次获取时并不能从远端的NameServer上拉取下来并更新本地缓存变量—topicPublishInfoTable成功。因此，第二次需要通过<strong>默认Topic—TBW102</strong>的TopicRouteData变量来构造TopicPublishInfo对象，并更新DefaultMQProducerImpl实例的本地缓存变量——topicPublishInfoTable。
另外，在该种类型的场景下，当消息发送至Broker代理服务器时，在<strong>SendMessageProcessor业务处理器的sendBatchMessage/sendMessage方法里面的super.msgCheck(ctx, requestHeader, response)消息前置校验中</strong>，会调用TopicConfigManager的createTopicInSendMessageMethod方法，<strong>在Broker端完成新Topic的创建并持久化至配置文件</strong>中（配置文件路径：{rocketmq.home.dir}/store/config/topics.json）。
 （2）<strong>生产者发送Topic已存在的消息</strong>：由于在NameServer中已经存在了该Topic，因此在第一次获取时候就能够取到并且更新至本地缓存变量中topicPublishInfoTable，随后tryToFindTopicPublishInfo方法直接可以return。</p>

<p><strong>2、选择消息发送的队列</strong></p>

<p>在获取了TopicPublishInfo路由信息后，RocketMQ的客户端在默认方式下，selectOneMessageQueuef()方法会从TopicPublishInfo中的messageQueueList中选择一个队列（MessageQueue）进行发送消息。具体的容错策略均在MQFaultStrategy这个类中定义。</p>

<p>在随机递增取模的基础上，再过滤掉not available的Broker代理。</p>

<p><strong>3、发送封装后的RemotingCommand数据包</strong></p>

<p>在选择完发送消息的队列后，RocketMQ就会调用sendKernelImpl()方法发送消息（该方法为，通过RocketMQ的Remoting通信模块真正发送消息的核心）。在该方法内总共完成以下几个步流程：
（1）根据前面获取到的MessageQueue中的brokerName，调用MQClientInstance实例的findBrokerAddressInPublish()方法，得到待发送消息中存放的Broker代理服务器地址，如果没有找到则更新路由信息；
（2）如果没有禁用，则发送消息前后会有钩子函数的执行（executeSendMessageHookBefore()/executeSendMessageHookAfter()方法）；
（3）将与该消息相关信息封装成RemotingCommand数据包，其中请求码RequestCode为以下几种之一：
 a.<strong>SEND_MESSAGE</strong>（普通发送消息）
 b.<strong>SEND_MESSAGE_V2</strong>（优化网络数据包发送）                                                                                                                     c.<strong>SEND_BATCH_MESSAGE</strong>（消息批量发送）
（4）根据获取到的Broke代理服务器地址，将封装好的RemotingCommand数据包发送对应的Broker上，默认发送超时间为3s；
（5）这里，真正调用RocketMQ的Remoting通信模块完成消息发送是在MQClientAPIImpl实例sendMessageSync()方法中；</p>

<p>（6）processSendResponse方法对发送正常和异常情况分别进行不同的处理并返回<strong>sendResult</strong>对象；
（7）发送返回后，调用updateFaultItem更新Broker代理服务器的可用时间；
（8）对于异常情况，且<strong>标志位—retryAnotherBrokerWhenNotStoreOK</strong>，设置为true时，在发送失败的时候，会<strong>选择换一个Broker</strong>；</p>

<h4 id="3-consumer">3、Consumer</h4>

<p><strong>1、MQ中Pull和Push的两种消费方式</strong></p>

<p>对于任何一款消息中间件而言，消费者客户端一般有两种方式从消息中间件获取消息并消费：
 （1）<strong>Push方式</strong>：由消息中间件（MQ消息服务器代理）主动地将消息推送给消费者；采用Push方式，可以尽可能实时地将消息发送给消费者进行消费。但是，在消费者的处理消息的能力较弱的时候(比如，消费者端的业务系统处理一条消息的流程比较复杂，其中的调用链路比较多导致消费时间比较久。概括起来地说就是<strong>“慢消费问题”</strong>)，而MQ不断地向消费者Push消息，消费者端的缓冲区可能会溢出，导致异常；
 （2）<strong>Pull方式</strong>：由消费者客户端主动向消息中间件（MQ消息服务器代理）拉取消息；采用Pull方式，如何设置Pull消息的频率需要重点去考虑，举个例子来说，可能1分钟内连续来了1000条消息，然后2小时内没有新消息产生（概括起来说就是<strong>“消息延迟与忙等待”</strong>）。如果每次Pull的时间间隔比较久，会增加消息的延迟，即消息到达消费者的时间加长，MQ中消息的堆积量变大；若每次Pull的时间间隔较短，但是在一段时间内MQ中并没有任何消息可以消费，那么会产生很多无效的Pull请求的RPC开销，影响MQ整体的网络性能；</p>

<p><strong>2、RocketMQ消息消费的长轮询机制</strong></p>

<p>如果长时间没有消息，而消费者端又不停的发送Pull请求不就会导致RocketMQ中Broker端负载很高吗？那么在RocketMQ中如何解决以做到高效的消息消费呢？</p>

<p>RocketMQ的消费方式都是基于<strong>Pull</strong>模式拉取消息的，而在这其中有一种<strong>长轮询机制</strong>（对普通轮询的一种优化），来平衡上面Push/Pull模型的各自缺点。基本设计思路是：消费者如果第一次尝试Pull消息失败（比如：Broker端没有可以消费的消息），Broker 并不立即给消费者客户端返回Response的响应，而是<strong>先hold住并且挂起请求</strong>（将请求保存至<strong>pullRequestTable</strong>本地缓存变量中），然后<strong>Broker端</strong>的后台独立线程—<strong>PullRequestHoldService</strong>会从pullRequestTable本地缓存变量中不断地去取，具体的做法是查询待拉取消息的偏移量是否小于消费队列最大偏移量，如果条件成立则说明有新消息达到Broker端（这里，在RocketMQ的<strong>Broker端</strong>会有一个后台独立线程—<strong>ReputMessageService</strong>不停地构建ConsumeQueue/IndexFile数据，同时取出hold住的请求并进行二次处理），则通过重新调用一次业务处理器—PullMessageProcessor的处理请求方法—processRequest()来重新尝试拉取消息（此处，每隔5S重试一次，默认长轮询整体的时间设置为30s）。
RocketMQ消息Pull的长轮询机制的关键在于Broker端的<strong>PullRequestHoldService</strong>和<strong>ReputMessageService</strong>两个后台线程。</p>

<p><strong>3、RocketMQ中消费者Push方式的启动流程</strong></p>

<p>Push方式的Consumer启动流程完成的任务比较多，主要任务如下：
 （1）设置consumerGroup、NameServer服务地址、消费起始偏移地址并根据参数Topic构建Consumer端的SubscriptionData（订阅关系值）；
 （2）在Consumer端注册消费者<strong>监听器</strong>，当消息到来时完成消费消息；
 （3）启动defaultMQPushConsumerImpl实例，主要完成前置校验、复制订阅关系（将defaultMQPushConsumer的订阅关系复制至<strong>rebalanceImpl</strong>中，包括retryTopic（重试主题）对应的订阅关系）、创建MQClientInstance实例、设置rebalanceImpl的各个属性值、pullAPIWrapper包装类对象的初始化、<strong>初始化offsetStore实例并加载消费进度</strong>、启动消息消费服务线程以及在MQClientInstance中注册consumer等任务；
 （4）启动MQClientInstance实例，其中包括完成客户端网络通信线程、拉取消息服务线程、负载均衡服务线程和若干个定时任务的启动；
 （5）向所有的Broker端发送心跳（采用加锁方式）；
 （6）最后，唤醒负载均衡服务线程在Consumer端开始负载均衡；</p>

<p><strong>4、RocketMQ中Pull和Push两种消费模式流程简析</strong></p>

<p>RocketMQ提供了两种消费模式，Push和Pull，<strong>大多数场景</strong>使用的是<strong>Push模式</strong>，在源码中这两种模式分别对应的是DefaultMQPushConsumer类和DefaultMQPullConsumer类。Push模式实际上在内部还是使用的Pull方式实现的，通过Pull不断地轮询Broker获取消息，当不存在新消息时，Broker端会挂起Pull请求，直到有新消息产生才取消挂起，返回新消息。
 <strong>（1）RocketMQ的Pull消费模式流程简析</strong>
RocketMQ的Pull模式相对来得简单，从上面的demo代码中可以看出，业务应用代码通过由Topic获取到的MessageQueue直接拉取消息（最后真正执行的是PullAPIWrapper的pullKernelImpl()方法，通过发送拉取消息的RPC请求给Broker端）。其中，<strong>消息消费的偏移量需要Consumer端自己去维护</strong>。
 <strong>（2）RocketMQ的Push消费模式流程简析</strong>
在本文前面已经提到过了，从严格意义上说，RocketMQ并没有实现真正的消息消费的Push模式，而是<strong>对Pull模式进行了一定的优化</strong>。</p>

<p>一方面在Consumer端开启后台独立的线程—<strong>PullMessageService</strong>不断地从阻塞队列—<strong>pullRequestQueue</strong>中获取PullRequest请求并通过网络通信模块发送Pull消息的RPC请求给Broker端。另外一方面，后台独立线程—<strong>rebalanceService</strong>根据Topic中消息队列个数和当前消费组内消费者个数进行<strong>负载均衡</strong>，将产生的对应<strong>PullRequest</strong>实例放入阻塞队列—<strong>pullRequestQueue</strong>中。这里算是比较典型的生产者-消费者模型，实现了准实时的自动消息拉取。然后，再根据业务反馈是否成功消费来推动消费进度。</p>

<p>在Broker端，<strong>PullMessageProcessor</strong>业务处理器收到Pull消息的RPC请求后，通过MessageStore实例从commitLog获取消息。如果第一次尝试Pull消息失败（比如Broker端没有可以消费的消息），则通过<strong>长轮询</strong>机制先hold住并且挂起该请求，然后通过Broker端的后台线程 <strong>PullRequestHoldService</strong> 重新尝试和后台线程ReputMessageService的二次处理。</p>

<p>消费者如果第一次尝试Pull消息失败（比如：Broker端没有可以消费的消息），Broker 并不立即给消费者客户端返回 Response 的响应，而是<strong>先hold住并且挂起请求</strong>（将请求保存至<strong>pullRequestTable</strong>本地缓存变量中），然后<strong>Broker端</strong>的后台独立线程—<strong>PullRequestHoldService</strong>会从pullRequestTable本地缓存变量中不断地去取，具体的做法是查询待拉取消息的偏移量是否小于消费队列最大偏移量，如果条件成立则说明有新消息达到Broker端（这里，在RocketMQ的<strong>Broker端</strong>会有一个后台独立线程—<strong>ReputMessageService</strong>不停地构建ConsumeQueue/IndexFile数据，同时取出hold住的请求并进行二次处理），则通过重新调用一次业务处理器—<strong>PullMessageProcessor</strong> 的处理请求方法—processRequest()来重新尝试拉取消息（此处，每隔5S重试一次，默认长轮询整体的时间设置为30s）。</p>

<p><strong>4、Consumer的push模式实现</strong></p>

<h5 id="1-rocketmq中长轮询的pull消息机制">1、RocketMQ中长轮询的Pull消息机制</h5>

<p>Consumer如果第一次尝试Pull消息失败（比如：Broker端没有可以消费的消息），并不立即给消费者客户端返回Response的响应，而是先hold住并且挂起请求。然后在Broker端，通过后台独立线程—<strong>PullRequestHoldService</strong>重复尝试执行Pull消息请求来取消息。同时，另外一个<strong>ReputMessageService</strong>线程不断地构建ConsumeQueue/IndexFile数据，并取出hold住的Pull请求进行二次处理。通过这种长轮询机制，即可解决Consumer端需要通过不断地发送无效的轮询Pull请求，而导致整个RocketMQ集群中Broker端负载很高的问题。</p>

<h5 id="2-consumer向broker端发送pull消息请求的主要过程">2、Consumer向Broker端发送Pull消息请求的主要过程</h5>

<p>在RocketMQ的Consumer端，后台独立线程服务—pullMessageService是Pull消息请求的发起者，它不断地尝试从阻塞队列—LinkedBlockingQueue&lt;PullRequest&gt;中获取元素PullRequest，并根据pullRequest中的参数以及订阅关系信息调用pullAPIWrapper的pullKernelImpl()方法发送封装后的Pull消息请求—PullMessageRequestHeader至Broker端来拉取消息（具体完成发送一次Pull消息的PRC通信请求的是MQClientAPIImpl中的pullMessage()方法）。这里涉及细节的时序图（ps：时序图中没有涉及PRC异步通信中的callback过程）如下：</p>

<p><img src="https://upload-images.jianshu.io/upload_images/4325076-21898e5e414be41a.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" /></p>

<p>其中， DefaultMQPushConsumerImpl的pullMessage(pullRequest)方法是发送Pull消息请求的关键：
 （1）校验ProcessQueue是否“drop”， 如果“drop”为true则直接返回（这个“drop”的设置在下面一节—“Consumer端的负载均衡机制”中会提到）；
 （2）给ProcessQueue设置Pull消息的时间戳；
 （3）做流量控制，对于满足下面条件的任何一种情况，稍后再发起Pull消息的请求；
 <strong>条件1</strong>：正在消费的队列中，未被消费的消息数和消息大小超过阀值（默认每个队列消息数为1000个/消息存储容量100MB）；
 <strong>条件2</strong>：如果是顺序消费，正在消费的队列中，消息的跨度超过阀值（默认2000）；
 （4）根据 <strong>topic</strong> 获取订阅关系 — SubscriptionData；
 （5）构建Pull消息的回调对象 — PullBack，这里从Broker端Pull消息的返回结果处理是通过异步回调（发送异步通信RPC请求），其中如果Broker端返回Pull消息成功，在回调方法中先填充至处理队列—<strong>processQueue</strong>中（将Pull下来的消息，设置到ProcessQueue的<strong>msgTreeMap</strong>容器中），然后通过消费消息的服务线程—<strong>consumeMessageService</strong>，将封装好的ConsumeRequest提交至消费端消费线程池—<strong>consumeExecutor</strong>异步执行处理（具体处理逻辑：通过业务应用系统在DefaultMQPushConsumer实例中注册的消息监听器完成业务端的消息消费）；
 （6）从Consumer端<strong>内存</strong>中获取commit<strong>Offset</strong>Value；
 （7）通过RocketMQ的Remoting通信层向Broker端发送Pull消息的RPC请求。</p>

<h5 id="3-broker端处理pull消息请求的一般过程">3、Broker端处理Pull消息请求的一般过程</h5>

<p>这里先来说下对于一般情况下（即为所要Pull的消息在RocketMQ的Broker端已经是存在，一般可以Pull到的情况），Broker端处理这个Pull消息请求的主要过程。其时序图（ps：图中只是画了大部分的流程，详细细节还需要对照源码看下）如下：</p>

<p><img src="https://upload-images.jianshu.io/upload_images/4325076-d940e151c944e8aa.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/965/format/webp" alt="img" /></p>

<p>从上面的简易时序图中可以看到Broker端Pull消息的主要关键点如下：
 （1）Pull消息的业务处理器—PullMessageProcessor的processRequest为处理拉取消息请求的入口，在设置reponse返回结果中的opaque值后，就完成一些前置的校验（Broker是否可读、Topic/ConsumerGroup是否存在、读取队列Id是否在Topic配置的队列范围数内）；
 （2）根据“ConsumerGroup”、“Topic”、“queueId”和“offset”这些参数来调用MessageStore实例的getMessage()方法来产尝试读取Broker端的消息；
 （3）其中，通过findConsumeQueue()方法，获取逻辑消费队列 — ConsumeQueue；
 （4）根据offset与逻辑消费队列中的maxOffset、minOffset的比较，来设置状态值status，同时计算出下次Pull消息的开始偏移量值—nextBeginOffset，然后通过<strong>MappedFile</strong>的方式获取ConsumeQueue的Buffer映射结果值；
 （5）根据算出来的offsetPy（物理偏移量值）和sizePy（消息的物理大小），从commitLog获取对应消息的Buffer 映射结果值，并填充至 <strong>GetMessageResult</strong> 返回对象，并设置返回结果（状态/下次其实偏移量/maxOffset/minOffset）后return；
 （6）根据isTransferMsgByHeap的设置情况（默认为true），选择下面两种方式之一来真正读取GetMessageResult的消息内容并返回至Consumer端；
 <strong>方式1</strong>：使用JDK <strong>NIO</strong>的ByteBuffer，循环地读取存有消息内容的messageBufferList至堆内内存中，返回byte[]字节数组，并设置到响应的body中；然后，通过RPC通信组件—NettyRemotingServer发送响应至Consumer端；
 <strong>方式2</strong>：采用基于<strong>Zero-Copy</strong>的Netty组件的FileRegion，其包装的“FileChannel.tranferTo”实现文件传输，可以直接将文件缓冲区的数据发送至通信目标通道Channel中，避免了通过循环write方式导致的内存拷贝开销，这种方式性能上更优；
 （7）在PullMessageProcessor业务处理器的最后，提交并持久化消息消费的offset偏移量进度。</p>

<h5 id="4-broker端对于pull请求挂起处理的流程">4、Broker端对于Pull请求挂起处理的流程</h5>

<p>长轮询机制是对普通轮询的一种优化方案，它平衡了传统Push/Pull模型的各自缺点，Server端如果当前没有Client端请求拉取的相关数据会<strong>hold</strong>住这个请求，直到Server端存在相关的数据，或者等待超时时间后返回。在响应返回后，Client端又会再次发起下一次的长轮询请求。RocketMQ的push模式正是采用了这种长轮询机制的设计思路，如果在上面所述的第一次尝试Pull消息失败后（比如Broker端暂时没有可以消费的消息），先hold住并且挂起该请求（这里，设置返回响应response为null，此时不会向Consumer端发送任何响应的内容，即不会对响应结果进行处理），然后通过Broker端的后台线程PullRequestHoldService重新尝试和后台线程ReputMessageService的二次处理。在Broker端，两个后台线程服务PullRequestHoldService和ReputMessageService是实现长轮询机制的关键点。下面就来分别介绍这两个服务线程：
 <strong>（1）PullRequestHoldService</strong>：该服务线程会从pullRequestTable本地缓存变量中取PullRequest请求，检查轮询条件—“<strong>待拉取消息的偏移量是否小于消费队列最大偏移量</strong>”是否成立，如果条件成立则说明有新消息达到Broker端，则通过PullMessageProcessor的executeRequestWhenWakeup()方法重新尝试发起Pull消息的RPC请求（此处，每隔5S重试一次，默认长轮询整体的时间设置为30s）；
 <strong>（2）ReputMessageService</strong>：该服务线程会在Broker端不断地从数据存储对象—commitLog中解析数据并分发请求，随后构建出ConsumeQueue（逻辑消费队列）和IndexFile（消息索引文件）两种类型的数据。同时从本地缓存变量—pullRequestTable中，取出hold住的PullRequest请求并执行二次处理（具体的做法是，在PullMessageProcessor的executeRequestWhenWakeup()方法中，通过业务线程池pullMessageExecutor，异步提交重新Pull消息的请求任务，即为重新调了一次PullMessageProcessor业务处理器的processRequest()方法，来实现Pull消息请求的二次处理）。这里，ReputMessageService服务线程，每处理一次，Thread.sleep(1)，继续下一次处理。</p>

<h5 id="5-consumer端的负载均衡机制">5、Consumer端的负载均衡机制</h5>

<p>在RocketMQ中，Consumer端的两种消费模式（Push/Pull）都是基于拉模式来Pull消息的，而在Push模式中只是采用了长轮询的方式而实现了准实时的自动消息拉取。在两种基于拉模式的消费方式（Push/Pull）中，均需要Consumer端在知道从Broker端的哪一个消息队列—MessageQueue中去Pull消息。因此，消息队列的负载均衡处理（即Broker端中多个MessageQueue分配给同一个ConsumerGroup中的哪些Consumer消费），由<strong>Consumer端来主动</strong>完成更为合理。</p>

<p><strong>1. Consumer端的心跳包发送</strong></p>

<p>在Consumer启动后，它就会通过定时任务不断地向RocketMQ集群中的所有Broker实例发送心跳包（其中包含了，消息消费分组名称、订阅关系集合、消息通信模式和客户端id的值等信息）。Broker端在收到Consumer的心跳消息后，会将它维护在ConsumerManager的本地缓存变量—<strong>consumerTable</strong>，同时将封装后的<strong>客户端网络通道信息</strong>保存在本地缓存变量—<strong>channelInfoTable</strong>中，为之后做Consumer端的负载均衡<strong>提供可以依据的元数据</strong>信息。
 <strong>2. Consumer端实现负载均衡的核心类—RebalanceImpl</strong>
 在上一篇文章的&quot;Consumer启动流程&quot;中已经介绍了在启动MQClientInstance实例时候，会完成负载均衡服务线程 — RebalanceService的启动（每隔20s执行一次）。通过查看源码可以发现，<strong>RebalanceService</strong>线程的run()方法最终调用的是RebalanceImpl类的rebalanceByTopic()方法，该方法是实现Consumer端负载均衡的核心关键。
这里，<strong>rebalanceByTopic()</strong>方法会根据消费者通信类型为“广播模式”还是“集群模式”做不同的逻辑处理。这里主要来看下集群模式下的主要处理流程：</p>

<p>（1）从rebalanceImpl实例的本地缓存变量 — <strong>topicSubscribeInfoTable</strong>中，获取该Topic主题下的消息消费队列集合（mqSet）；</p>

<p>（2）根据topic和consumerGroup为参数调用mQClientFactory.findConsumerIdList()方法向Broker端发送获取<strong>该消费组下消费者Id列表</strong>的RPC通信请求（Broker端基于前面Consumer端上报的心跳包数据而构建的<strong>consumerTable</strong>做出响应返回，业务请求码：GET_CONSUMER_LIST_BY_GROUP）；</p>

<p>（3）先对Topic下的消息消费队列、消费者Id排序，然后用消息队列分配策略算法（默认为：消息队列的平均分配算法），计算出待拉取的消息队列。</p>

<p><img src="https:////upload-images.jianshu.io/upload_images/4325076-749f4399124ce8b4.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/571/format/webp" alt="img" />
这里的平均分配算法，类似于分页的算法，将所有MessageQueue排好序类似于记录，将所有消费端Consumer排好序类似页数，并求出每一页需要包含的平均size和每个页面记录的范围range，最后遍历整个range而计算出当前Consumer端应该分配到的记录 （这里即为：MessageQueue）。</p>

<p>（4）然后，调用updateProcessQueueTableInRebalance()方法，具体的做法是，先将分配到的消息队列集合（mqSet）与processQueueTable做一个过滤比对，具体的过滤比对方式如下图：</p>

<p><img src="https:////upload-images.jianshu.io/upload_images/4325076-5b5f0199f29aae25.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/897/format/webp" alt="img" /></p>

<p>这里可以分如下两种情况来筛选过滤：a.图中processQueueTable标注的红色部分，表示与分配到的消息队列集合mqSet互不包含。</p>

<p><strong>b.图中processQueueTable的绿色部分，表示与分配到的消息队列集合mqSet的交集。</strong>判断该ProcessQueue是否已经过期了，在Pull模式的不用管，如果是Push模式的，设置Dropped属性为true，并且调用removeUnnecessaryMessageQueue()方法，像上面一样尝试移除Entry；
最后，为过滤后的消息队列集合（mqSet）中的每个MessageQueue创建一个ProcessQueue对象并存入RebalanceImpl的processQueueTable队列中（其中调用RebalanceImpl实例的computePullFromWhere(MessageQueue mq)方法获取该MessageQueue对象的下一个进度消费值<strong>offset</strong>，随后填充至接下来要创建的pullRequest对象属性中），并创建拉取请求对象—pullRequest添加到拉取列表—pullRequestList中，最后执行dispatchPullRequest()方法，将Pull消息的请求对象PullRequest依次放入PullMessageService服务线程的阻塞队列pullRequestQueue中，待该服务线程取出后向Broker端发起Pull消息的请求。其中，可以重点对比下，RebalancePushImpl和RebalancePullImpl两个实现类的dispatchPullRequest()方法不同，RebalancePullImpl类里面的该方法为空。</p>

<h5 id="6-rocketmq消费失败后的消费重试机制">6、RocketMQ消费失败后的消费重试机制</h5>

<p><strong>“重试队列”和“死信队列”两个概念</strong></p>

<p><strong>（1）重试队列：</strong>如果Consumer端因为各种类型异常导致本次消费失败，为防止该消息丢失而需要将其重新回发给Broker端保存，保存这种因为异常无法正常消费而回发给MQ的消息队列称之为重试队列。RocketMQ会为<strong>每个消费组</strong>都设置一个Topic名称为<strong>“%RETRY%+consumerGroup”的重试队列</strong>（这里需要注意的是，<strong>这个Topic的重试队列是针对消费组，而不是针对每个Topic设置的</strong>），用于暂时保存因为各种异常而导致Consumer端无法消费的消息。考虑到异常恢复起来需要一些时间，会为重试队列设置多个重试级别，每个重试级别都有与之对应的重新投递延时，重试次数越多投递延时就越大。RocketMQ对于重试消息的处理是先保存至Topic名称为<strong>“SCHEDULE_TOPIC_XXXX”</strong>的延迟队列中，后台定时任务按照对应的时间进行Delay后重新保存至<strong>“%RETRY%+consumerGroup”</strong>的重试队列中。</p>

<p><strong>（2）死信队列：</strong>由于有些原因导致Consumer端长时间的无法正常消费从Broker端Pull过来的业务消息，为了确保消息不会被无故的丢弃，那么超过配置的“最大重试消费次数”后就会移入到这个死信队列中。在RocketMQ中，SubscriptionGroupConfig配置常量默认地设置了两个参数，一个是retryQueueNums为1（重试队列数量为1个），另外一个是retryMaxTimes为16（最大重试消费的次数为16次）。Broker端通过校验判断，如果超过了最大重试消费次数则会将消息移至这里所说的死信队列。这里，RocketMQ会为每个消费组都设置一个Topic命名为<strong>“%DLQ%+consumerGroup&quot;的死信队列</strong>。一般在实际应用中，移入至死信队列的消息，需要<strong>人工干预处理</strong>；</p>

<p><strong>1. Consumer端回发消息至Broker端</strong></p>

<p>在业务工程中的Consumer端（Push消费模式下），如果消息能够正常消费需要在注册的消息监听回调方法中返回<strong>CONSUME_SUCCESS</strong>的消费状态，否则因为各类异常消费失败则返回<strong>RECONSUME_LATER</strong>的消费状态。</p>

<p>如果业务工程对消息消费失败了，那么则会抛出异常并且返回这里的<strong>RECONSUME_LATER</strong>状态。这里，在消费消息的服务线程—consumeMessageService中，将封装好的消息消费任务ConsumeRequest提交至线程池—consumeExecutor异步执行。从消息消费任务ConsumeRequest的run()方法中会执行业务工程中注册的消息监听回调方法，并在processConsumeResult方法中根据业务工程返回的状态（CONSUME_SUCCESS或者RECONSUME_LATER）进行判断和做对应的处理（下面讲的都是在消费通信模式为集群模型下的，广播模型下的比较简单就不再分析了）。
 <strong>（1）业务方正常消费（CONSUME_SUCCESS）</strong>：正常情况下，<strong>Consumer</strong>设置 ackIndex 的值为consumeRequest.getMsgs().size() - 1，因此后面的遍历consumeRequest.getMsgs()消息集合条件不成立，不会调用回发消费失败消息至Broker端的方法—sendMessageBack(msg, context)。最后，更新消费的偏移量；</p>

<p><strong>（2）业务方消费失败（RECONSUME_LATER）</strong>：异常情况下，<strong>Consumer</strong>设置 ackIndex 的值为-1，这时就会进入到遍历consumeRequest.getMsgs()消息集合的for循环中，执行回发消息的方法—sendMessageBack(msg, context)。这里，首先会根据brokerName得到 Broker 端的地址信息，然后通过网络通信的Remoting模块发送RPC请求到指定的Broker上，如果上述过程失败，则创建一条新的消息重新发送给Broker，此时新消息的Topic为<strong>“%RETRY%+ConsumeGroupName”</strong>—重试队列的主题。其中，在MQClientAPIImpl实例的consumerSendMessageBack() 方法中封装了 ConsumerSendMsgBackRequestHeader 的请求体，随后完成回发消费失败消息的RPC通信请求（<strong>业务请求码为：CONSUMER_SEND_MSG_BACK</strong>）。倘若上面的回发消息流程失败，则会延迟5S后重新在Consumer端进行重新消费。与正常消费的情况一样，在最后更新消费的偏移量。</p>

<p><strong>2. Broker端对于回发消息处理的主要流程</strong></p>

<p>Broker端收到这条Consumer端回发过来的 CONSUMER_SEND_MSG_BACK 消息后，现将<strong>延迟队列id，comsumerqueueID以及offset</strong>保存至SCHEDULE_TOPIC_XXXX，到时间了再保存至%RETRY%+consumerGroup。DeliverDelayedMessageTimerTask<strong>定时任务线程</strong>会根据Topic（“SCHEDULE_TOPIC_XXXX”）与QueueId，先查到逻辑消费队列ConsumeQueue，然后根据偏移量，找到ConsumeQueue中的内存映射对象，从commitlog日志中找到消息对象MessageExt，并做一个消息体的<strong>转换</strong>（messageTimeup()方法，由定时延迟队列消息转化为重试队列的消息），再次做持久化落盘，这时候才会真正的保存至重试队列中。</p>

<p>Broker端收到这条Consumer端回发过来的消息后，通过业务请求码（<strong>CONSUMER_SEND_MSG_BACK</strong>）匹配业务处理器—SendMessageProcessor来处理。在完成一系列的前置校验（这里主要是“消费分组是否存在”、“检查Broker是否有写入权限”、“检查重试队列数是否大于0”等）后，尝试获取重试队列的TopicConfig对象（如果是第一次无法获取到，则调用createTopicInSendMessageBackMethod()方法进行创建）。根据回发过来的消息偏移量尝试从commitlog日志文件中查询消息内容，若不存在则返回异常错误。</p>

<p>然后，设置重试队列的Topic—<strong>“%RETRY%+consumerGroup”</strong> 至MessageExt的扩展属性“RETRY_TOPIC”中，并对根据延迟级别delayLevel和最大重试消费次数 maxReconsumeTimes 进行判断，如果超过最大重试消费次数（默认16次），则会创建死信队列的TopicConfig对象（用于后面将回发过来的消息移入死信队列）。在构建完成需要落盘的MessageExtBrokerInner对象后，调用“commitLog.putMessage(msg)”方法做消息持久化。这里，需要注意的是，在putMessage(msg)的方法里会使用<strong>“SCHEDULE_TOPIC_XXXX”</strong>和对应的延迟级别队列Id分别替换MessageExtBrokerInner 对象的 Topic 和 QueueId 属性值，并将原来设置的重试队列主题（<strong>“%RETRY%+consumerGroup”</strong>）的 Topic 和 QueueId 属性值做一个备份分别存入扩展属性properties的“REAL_TOPIC”和“REAL_QID”属性中。看到这里也就大致明白了，回发给Broker端的消费失败的消息并非直接保存至重试队列中，而是会先存至Topic为<strong>“SCHEDULE_TOPIC_XXXX”</strong>的定时延迟队列中。</p>

<blockquote>
<p>疑问：上面说了RocketMQ的重试队列的Topic是“%RETRY%+consumerGroup”，为啥这里要保存至Topic是“SCHEDULE_TOPIC_XXXX”的这个延迟队列中呢？</p>
</blockquote>

<p>在源码中搜索下关键字—“SCHEDULE_TOPIC_XXXX”，会发现Broker端还存在着一个后台服务线程—ScheduleMessageService（通过消息存储服务—DefaultMessageStore启动），通过查看源码可以知道其中有一个DeliverDelayedMessageTimerTask<strong>定时任务线程</strong>会根据Topic（“SCHEDULE_TOPIC_XXXX”）与QueueId，先查到逻辑消费队列ConsumeQueue，然后根据偏移量，找到ConsumeQueue中的内存映射对象，从commitlog日志中找到消息对象MessageExt，并做一个消息体的<strong>转换</strong>（messageTimeup()方法，由定时延迟队列消息转化为重试队列的消息），再次做持久化落盘，这时候才会真正的保存至重试队列中。看到这里就可以解释上面的疑问了，定时延迟队列只是为了用于暂存的，然后延迟一段时间再将消息移入至重试队列中。RocketMQ设定不同的延时级别delayLevel，并且与定时延迟队列相对应。</p>

<p><strong>3. Consumer端消费重试机制</strong></p>

<p>每个Consumer实例在启动的时候就默认订阅了该消费组的重试队列主题，在DefaultMQPushConsumerImpl的copySubscription()方法中。</p>

<p>Consumer端会一直订阅该重试队列主题的消息，向Broker端发送如下的拉取消息的PullRequest请求，以尝试重新再次消费重试队列中积压的消息。</p>

<h4 id="4-nameserver">4、NameServer</h4>

<p>主要提供两个功能：<strong>Broker管理</strong> 和 <strong>路由信息管理</strong> 。说白了就是 <code>Broker</code> 会将自己的信息注册到 <code>NameServer</code> 中，此时 <code>NameServer</code> 就存放了很多 <code>Broker</code> 的信息(Broker的路由表)，消费者和生产者就从 <code>NameServer</code> 中获取路由表然后照着路由表的信息和对应的 <code>Broker</code> 进行通信(生产者和消费者定期会向 <code>NameServer</code> 去查询相关的 <code>Broker</code> 的信息)。</p>

<p><code>NameServer</code> 也做了集群部署，但是请注意它是 <strong>去中心化</strong> 的。也就意味着它没有主节点，你可以很明显地看出 <code>NameServer</code> 的所有节点是没有进行 <code>Info Replicate</code> 的，在 <code>RocketMQ</code> 中是通过 <strong>单个Broker和所有NameServer保持长连接</strong> ，并且在每隔30秒 <code>Broker</code> 会向<strong>所有</strong> <code>Nameserver</code> 发送心跳，心跳包含了自身的 <code>Topic</code> 配置信息，这个步骤就对应这上面的 <code>Routing Info</code> 。</p>

<p>在生产者需要向 <code>Broker</code> 发送消息的时候，<strong>需要先从 <code>NameServer</code> 获取关于 <code>Broker</code> 的路由信息</strong>，然后通过 <strong>轮询</strong> 的方法去向每个队列中生产数据以达到 <strong>负载均衡</strong> 的效果。</p>

<p>消费者通过 <code>NameServer</code> 获取所有 <code>Broker</code> 的路由信息后，向 <code>Broker</code> 发送 <code>Pull</code> 请求来获取消息数据。</p>

<h4 id="5-deldger">5、Deldger</h4>

<p>传统的不支持动态 Leader 选举，可以保证消息不丢失。Leader 宕机了可以切换到其他 Leader 节点，但无法保证顺序性。</p>

<p>DLeager 要求至少消息复制到半数以上的节点之后，才给客户端返回写入成功，并且它是支持通过选举来动态切换 Leader 的。 但选举过程中不能提供服务。</p>

<p><strong>传统的复制方式</strong></p>

<p>在 RocketMQ 中，异步发消息如果在返回“写入成功”前，需要写入的副本数不够多，那就会丢消息。</p>

<p>对 RocketMQ 来说，如果采用异步复制的方式会不会丢消息呢？</p>

<p>答案是，并不会丢消息。在 RocketMQ 中，Broker 的主从关系是通过配置固定的，不支持动态切换。如果主节点宕机，生产者就不能再生产消息了，消费者可以自动切换到从节点继续进行消费。这时候，即使有一些消息没有来得及复制到从节点上，这些消息依然躺在主节点的磁盘上，除非是主节点的磁盘坏了，否则等主节点重新恢复服务的时候，这些消息依然可以继续复制到从节点上，也可以继续消费，不会丢消息，消息的顺序也是没有问题的。</p>

<p>从设计上来讲，RocketMQ 的这种主从复制方式，牺牲了可用性，换取了比较好的性能和数据一致性。</p>

<p>那 RocketMQ 又是如何解决可用性的问题的呢？RocketMQ 支持把一个主题分布到多对主从节点上去，每对主从节点中承担主题中的一部分队列，如果某个主节点宕机了，会<strong>自动切换到其他主节点</strong>上继续发消息，这样既解决了可用性的问题，还可以通过水平扩容来提升 Topic 总体的性能。这种复制方式在大多数场景下都可以很好的工作，但也面临一些问题。</p>

<p>比如，在需要保证消息严格顺序的场景下，由于在主题层面无法保证严格顺序，所以必须指定队列来发送消息，对于任何一个队列，它一定是落在一组特定的主从节点上，如果这个主节点宕机，其他的主节点是无法替代这个主节点的，否则就<strong>无法保证严格顺序</strong>。在这种复制模式下，严格顺序和高可用只能选择一个。</p>

<p><strong>Dledger</strong></p>

<p>Dledger 在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客户端返回写入成功，并且它是支持通过选举来动态切换主节点的。</p>

<p>拿 3 个节点举例说明一下。当主节点宕机的时候，2 个从节点会通过投票选出一个新的主节点来继续提供服务，相比主从的复制模式，解决了可用性的问题。由于消息要至少复制到 2 个节点上才会返回写入成功，即使主节点宕机了，也至少有一个节点上的消息是和主节点一样的。Dledger 在选举时，总会把数据和主节点一样的从节点选为新的主节点，这样就保证了数据的一致性，既不会丢消息，还可以保证严格顺序。</p>

<p>Dledger 的复制方式也不是完美的，依然存在一些不足：比如，选举过程中不能提供服务。最少需要 3 个节点才能保证数据一致性，3 节点时，只能保证 1 个节点宕机时可用，如果 2 个节点同时宕机，即使还有 1 个节点存活也无法提供服务，资源的利用率比较低。另外，由于至少要复制到半数以上的节点才返回写入成功，性能上也不如主从异步复制的方式快。</p>

<h4 id="6-事务">6、事务</h4>

<p>RocketMQ通过两阶段的方式提供事务消息的支持。</p>

<p>RocketMQ实现事务消息依赖于 TransactionListener 接口， 其中包含两个方法：</p>

<p>executeLocalTransaction 方法会在发送消息后调用，用于执行本地事务，如果本地事务执行成功，rocketmq 再提交消息；</p>

<p>checkLocalTransaction 用于对本地事务做检查。</p>

<p>然后将这个 TransactionListener 赋值给 producer，producer.sendMessageInTranction() 来执行事务。</p>

<p>RocketMQ通过<strong>两个内部的topic</strong>来实现对消息的两阶段支持，RocketMQ在实现事务消息时，实际上是通过将生产者投递过来的消息（消息上带有事务标识）投递到一个名为<strong>RMS_SYS_TRANS_HALF_TOPIC</strong>的topic中，而不是投递到真正的topic中，这个过程是第一阶段(prepare)，然后producer再通过TransactionListener的executeLocalTransaction方法执行本地事务，当producer的localTransaction处理成功或者失败后，producer会向broker发送commit或rollback命令，如果是commit，则broker会将投递到RMQ_SYS_TRANS_HALF_TOPIC中的消息投递到真实的topic中，然后再投递一个<strong>表示删除的消息</strong>到<strong>RMQ_SYS_TRANS_OP_HALF_TOPIC</strong>中，表示当前事务已完成；如果是rollback，则没有投递到真实topic的过程，只需要投递表示删除的消息到RMQ_SYS_TRANS_OP_HALF_TOPIC。最后，消费者和消费普通的消息一样消费事务消息。</p>

<p>整个过程如果没有遇到问题，则一切OK，但整个过程中可能会遇到以下错误：</p>

<ul>
<li>第一阶段（prepare）失败：给应用返回发送消息失败</li>
<li>事务失败：发送回滚命令给broker，由broker执行消息的回滚</li>
<li>Commit或rollback失败：由broker定时向producer发起事务检查，如果本地事务成功，则提交消息事务，否则回滚消息事务</li>
</ul>

<p>事务状态的检查有两种情况：</p>

<ul>
<li>commit/rollback：broker会执行相应的commit/rollback操作</li>
<li>如果是TRANSACTION_NOT_TYPE，则一段时间后会再次检查，当检查的次数超过上限（默认15次）则丢弃消息。</li>
</ul>

<h4 id="7-选择队列以及顺序消费">7、选择队列以及顺序消费</h4>

<p><strong>选择队列：</strong></p>

<p>Producer 在</p>

<p>producer.send(msg,  new MessageQueueSelector(  public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg)  ), arg ) 选择消息队列。</p>

<p>重写 MessageQueueSelector 的 select 方法返回指定的 MessageQueue。</p>

<p><strong>并发消费</strong>（线程池消费消息）：</p>

<p>consumer.registerMessageListener（ new MessageListener<strong>Concurrently</strong>() {</p>

<p>​       public ConsumeConcurrentlyStatus consumeMessage( final List&lt;MessageExt&gt; msgs, final ConsumeConcurrentlyContext context ) { 执行消息消费逻辑，成功返回 <strong>CONSUME_SUCCESS</strong>，失败返回<strong>RECONSUME_LATER</strong>  }</p>

<p>}）；</p>

<p>注册 MessageListener 是 Concurrently。</p>

<p>RECONSUME_LATER 将消息放到 <strong>重试队列</strong> 。先保存至 SCHEDULE_TOPIC_XXXX，定时任务按照延迟时间将消息保存到重试队列。</p>

<p><strong>顺序消费</strong>：</p>

<p>consumer.registerMessageListener（ new MessageListener<strong>Orderly</strong>() {</p>

<p>​       public ConsumeOrderlyStatus consumeMessage( final List&lt;MessageExt&gt; msgs, final ConsumeOrderlyContext context ) { 执行消息消费逻辑，成功返回 <strong>SUCCESS</strong>，失败返回 <strong>SUSPEND_CURRENT_QUEUE_A_MOMENT</strong>  }</p>

<p>}）；</p>

<p>注册 MessageListener 是 Orderly。</p>

<p>SUSPEND_CURRENT_QUEUE_A_MOMENT 挂起再消费消息。</p>
</div>
        <div class="post_footer">
          
          <div class="meta">
            <div class="info">
              <span class="field tags">
                <i class="remixicon-stack-line"></i>
                
                <a href="https://jixinhe111.github.io/tags/mq/">MQ</a>
                
              </span>
            </div>
          </div>
          
        </div>
      </div>
      
      
    </div>
  </div>
  <a id="back_to_top" href="#" class="back_to_top"><span>△</span></a>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://jixnhe111.tk">Finished by jxh,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span>真奇怪</span>
  </div>
</footer>



<script src="https://jixinhe111.github.io/js/jquery-3.3.1.min.js"></script>
<script src="https://jixinhe111.github.io/js/zozo.js"></script>
<script src="https://jixinhe111.github.io/js/highlight.pack.js"></script>
<link  href="https://jixinhe111.github.io/css/fancybox.min.css" rel="stylesheet">
<script src="https://jixinhe111.github.io/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>


  <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>






</body>
</html>
